{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbe58cdf",
   "metadata": {},
   "source": [
    "# üöÄ Knowledge Distillation for Reasoning Tasks\n",
    "\n",
    "## üìç Environment Support:\n",
    "- ‚úÖ **Google Colab** (Recommended for free GPU)\n",
    "- ‚úÖ **Kaggle** (Pre-installed packages)\n",
    "- ‚úÖ **Local** (Windows/Linux/Mac)\n",
    "\n",
    "---\n",
    "\n",
    "## üî• Google Colab Setup:\n",
    "\n",
    "### 1Ô∏è‚É£ Enable GPU:\n",
    "   - Click **Runtime** ‚Üí **Change runtime type**\n",
    "   - Hardware accelerator: **GPU** (T4 or better)\n",
    "   - Click **Save**\n",
    "\n",
    "### 2Ô∏è‚É£ Mount Google Drive (Optional but Recommended):\n",
    "   - Saves cache permanently (survives session restarts)\n",
    "   - Auto-prompted when you run cell 2\n",
    "   - Click **Connect to Google Drive** ‚Üí **Allow**\n",
    "\n",
    "### 3Ô∏è‚É£ Check GPU:\n",
    "   ```python\n",
    "   !nvidia-smi\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö° Quick Start:\n",
    "1. Run cells in order (Shift+Enter)\n",
    "2. First run: Extracts teacher cache (~30-60 min)\n",
    "3. Cache saved to Google Drive for reuse\n",
    "4. Subsequent runs: Much faster!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5269caa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Packages installed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Install packages for Google Colab\n",
    "!pip install -q transformers==4.36.0 peft==0.7.1 datasets==2.16.0 accelerate==0.25.0 bitsandbytes==0.41.3 wandb scikit-learn\n",
    "\n",
    "import os\n",
    "os.environ['WANDB_DISABLED'] = 'true'\n",
    "\n",
    "print(\"‚úÖ Packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81727a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: nvidia-smi: command not found\n",
      "\n",
      "======================================================================\n",
      "üî• PyTorch version: 2.9.0+cu126\n",
      "üî• CUDA available: False\n",
      "‚ö†Ô∏è  WARNING: GPU not detected!\n",
      "   In Colab: Runtime ‚Üí Change runtime type ‚Üí GPU\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# üîç Check GPU availability (especially important for Colab)\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"üî• PyTorch version: {torch.__version__}\")\n",
    "print(f\"üî• CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üî• CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"üî• GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"üî• GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  WARNING: GPU not detected!\")\n",
    "    print(\"   In Colab: Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7b6407e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üåç Environment: Kaggle\n",
      "üî• Device: cpu\n",
      "üî• GPU: N/A\n",
      "\n",
      "üìÇ Paths:\n",
      "   Output: /kaggle/working/distill_output\n",
      "   Cache: /kaggle/working/latent_cache\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    TaskType\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional\n",
    "import gc\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(token=\"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\")\n",
    "\n",
    "# Config - Auto-detect environment (Colab, Kaggle, or Local)\n",
    "class Config:\n",
    "    # Detect environment\n",
    "    IS_COLAB = 'google.colab' in str(get_ipython()) if 'get_ipython' in dir() else False\n",
    "    IS_KAGGLE = os.path.exists('/kaggle')\n",
    "    IS_LOCAL = not (IS_COLAB or IS_KAGGLE)\n",
    "    \n",
    "    # Models\n",
    "    TEACHER_MODEL = \"meta-llama/Llama-2-13b-hf\"\n",
    "    STUDENT_MODEL = \"mistralai/Mistral-7B-v0.1\"\n",
    "    \n",
    "    # Dataset\n",
    "    DATASET_NAME = \"gsm8k\"\n",
    "    DATASET_CONFIG = \"main\"\n",
    "    MAX_SAMPLES = 2000\n",
    "    MAX_LENGTH = 512\n",
    "    \n",
    "    # Training\n",
    "    BATCH_SIZE = 2\n",
    "    GRADIENT_ACCUM = 8\n",
    "    LEARNING_RATE = 2e-4\n",
    "    NUM_EPOCHS = 3\n",
    "    WARMUP_STEPS = 100\n",
    "    \n",
    "    # Distillation\n",
    "    ALPHA_OUTPUT = 0.5\n",
    "    BETA_LATENT = 0.5\n",
    "    TEMPERATURE = 2.0\n",
    "    LATENT_LAYERS = [8, 16, 24]\n",
    "    \n",
    "    # LoRA\n",
    "    LORA_R = 16\n",
    "    LORA_ALPHA = 32\n",
    "    LORA_DROPOUT = 0.05\n",
    "    LORA_TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "    \n",
    "    # Environment-specific paths\n",
    "    if IS_COLAB:\n",
    "        # Google Colab paths\n",
    "        OUTPUT_DIR = \"/content/drive/MyDrive/distill_output\"\n",
    "        LATENT_CACHE_DIR = \"/content/drive/MyDrive/latent_cache\"\n",
    "        USE_GDRIVE = True\n",
    "    elif IS_KAGGLE:\n",
    "        # Kaggle paths\n",
    "        OUTPUT_DIR = \"/kaggle/working/distill_output\"\n",
    "        LATENT_CACHE_DIR = \"/kaggle/working/latent_cache\"\n",
    "        USE_GDRIVE = False\n",
    "    else:\n",
    "        # Local paths\n",
    "        OUTPUT_DIR = \"./distill_output\"\n",
    "        LATENT_CACHE_DIR = \"./latent_cache\"\n",
    "        USE_GDRIVE = False\n",
    "    \n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Mount Google Drive if on Colab (to save cache permanently)\n",
    "if config.IS_COLAB and config.USE_GDRIVE:\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive')\n",
    "        print(\"‚úÖ Google Drive mounted successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not mount Google Drive: {e}\")\n",
    "        print(\"   Using /content/ instead (will be lost after session)\")\n",
    "        config.OUTPUT_DIR = \"/content/distill_output\"\n",
    "        config.LATENT_CACHE_DIR = \"/content/latent_cache\"\n",
    "        config.USE_GDRIVE = False\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(config.OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(config.LATENT_CACHE_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"üåç Environment: {'Google Colab' if config.IS_COLAB else 'Kaggle' if config.IS_KAGGLE else 'Local'}\")\n",
    "print(f\"üî• Device: {config.DEVICE}\")\n",
    "print(f\"üî• GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'N/A'}\")\n",
    "if config.IS_COLAB:\n",
    "    print(f\"üíæ Google Drive: {'Mounted ‚úÖ' if config.USE_GDRIVE else 'Not mounted ‚ö†Ô∏è'}\")\n",
    "print(f\"\\nüìÇ Paths:\")\n",
    "print(f\"   Output: {config.OUTPUT_DIR}\")\n",
    "print(f\"   Cache: {config.LATENT_CACHE_DIR}\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1881a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Loading GSM8K dataset...\n",
      "‚úÖ Train: 2000 | Test: 500\n",
      "‚úÖ Train: 2000 | Test: 500\n"
     ]
    }
   ],
   "source": [
    "def prepare_prompt(question: str, answer: str = None) -> str:\n",
    "    \"\"\"Format prompt for reasoning task\"\"\"\n",
    "    prompt = f\"Question: {question}\\n\\nLet's solve this step by step:\\n\"\n",
    "    \"\"\"This really need to be improved later\"\"\"\n",
    "    if answer:\n",
    "        prompt += f\"{answer}\"\n",
    "    return prompt\n",
    "\n",
    "class ReasoningDataset(Dataset):\n",
    "    \"\"\"Custom dataset with latent cache support\"\"\"\n",
    "    def __init__(self, data, tokenizer, max_length=512, latent_dir=None):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.latent_dir = latent_dir\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        # Tokenize\n",
    "        prompt = prepare_prompt(item['question'], item.get('answer'))\n",
    "        encoding = self.tokenizer(\n",
    "            prompt,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        result = {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'idx': idx\n",
    "        }\n",
    "        \n",
    "        # Load cached latent if available\n",
    "        if self.latent_dir:\n",
    "            latent_path = os.path.join(self.latent_dir, f\"latent_{idx}.pt\")\n",
    "            if os.path.exists(latent_path):\n",
    "                result['teacher_latents'] = torch.load(latent_path)\n",
    "        \n",
    "        return result\n",
    "\n",
    "# Load GSM8K dataset\n",
    "print(\"üì¶ Loading GSM8K dataset...\")\n",
    "dataset = load_dataset(config.DATASET_NAME, config.DATASET_CONFIG)\n",
    "\n",
    "# Sample subset for Kaggle\n",
    "train_data = dataset['train'].select(range(min(config.MAX_SAMPLES, len(dataset['train']))))\n",
    "test_data = dataset['test'].select(range(min(500, len(dataset['test']))))\n",
    "\n",
    "print(f\"‚úÖ Train: {len(train_data)} | Test: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bbc4719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üß† TEACHER KNOWLEDGE EXTRACTION\n",
      "======================================================================\n",
      "Cache status: 0/2000 files exist\n",
      "Will extract: 2000 samples\n",
      "üîÑ Loading Teacher Model (4-bit)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b1dc1bb9213472f9929a3946a7b199b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/610 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "No GPU found. A GPU is needed for quantization.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2811248760.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Stopped by user. Please mount Google Drive and restart.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m     \u001b[0mteacher_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteacher_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_teacher_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m     extract_latent_states(\n\u001b[1;32m    124\u001b[0m         \u001b[0mteacher_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-2811248760.py\u001b[0m in \u001b[0;36mload_teacher_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     )\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTEACHER_MODEL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mquantization_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbnb_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    567\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             )\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2895\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mload_in_8bit\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mload_in_4bit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2896\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No GPU found. A GPU is needed for quantization.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_accelerate_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_bitsandbytes_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m                 raise ImportError(\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No GPU found. A GPU is needed for quantization."
     ]
    }
   ],
   "source": [
    "def load_teacher_model():\n",
    "    \"\"\"Load teacher with 4-bit quantization to save memory\"\"\"\n",
    "    print(\"üîÑ Loading Teacher Model (4-bit)...\")\n",
    "    \n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        config.TEACHER_MODEL,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.TEACHER_MODEL)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "def check_cache_completeness(cache_dir, data_size):\n",
    "    \"\"\"Check how many cache files exist\"\"\"\n",
    "    if not os.path.exists(cache_dir):\n",
    "        return 0\n",
    "    \n",
    "    existing_files = [f for f in os.listdir(cache_dir) if f.startswith('latent_') and f.endswith('.pt')]\n",
    "    return len(existing_files)\n",
    "\n",
    "def extract_latent_states(model, tokenizer, data, output_dir, batch_size=1):\n",
    "    \"\"\"Extract and cache teacher's latent states\"\"\"\n",
    "    print(f\"üß† Extracting latent states to {output_dir}...\")\n",
    "    \n",
    "    # Check existing cache\n",
    "    existing_count = check_cache_completeness(output_dir, len(data))\n",
    "    print(f\"   Found {existing_count}/{len(data)} existing cache files\")\n",
    "    \n",
    "    if existing_count == len(data):\n",
    "        print(\"‚úÖ All cache files exist! Skipping extraction.\")\n",
    "        return\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Estimate time\n",
    "    print(f\"\\n‚è±Ô∏è  Estimated time: ~{len(data) - existing_count} samples √ó 2-3 sec = {(len(data) - existing_count) * 2.5 / 60:.1f} min\")\n",
    "    print(f\"   Progress will be saved to: {output_dir}\")\n",
    "    if config.USE_GDRIVE:\n",
    "        print(f\"   üíæ Cache saved to Google Drive (permanent)\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  Cache in /content/ (lost after session ends)\")\n",
    "    \n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx in tqdm(range(len(data)), desc=\"Extracting\"):\n",
    "            cache_path = os.path.join(output_dir, f\"latent_{idx}.pt\")\n",
    "            \n",
    "            # Skip if already cached\n",
    "            if os.path.exists(cache_path):\n",
    "                continue\n",
    "            \n",
    "            item = data[idx]\n",
    "            prompt = prepare_prompt(item['question'], item.get('answer'))\n",
    "            \n",
    "            inputs = tokenizer(\n",
    "                prompt,\n",
    "                return_tensors='pt',\n",
    "                truncation=True,\n",
    "                max_length=config.MAX_LENGTH\n",
    "            ).to(model.device)\n",
    "            \n",
    "            # Forward pass with hidden states\n",
    "            outputs = model(\n",
    "                **inputs,\n",
    "                output_hidden_states=True,\n",
    "                return_dict=True\n",
    "            )\n",
    "            \n",
    "            # Extract specific layers\n",
    "            latent_states = {}\n",
    "            for layer_idx in config.LATENT_LAYERS:\n",
    "                if layer_idx < len(outputs.hidden_states):\n",
    "                    # Average pool over sequence\n",
    "                    hidden = outputs.hidden_states[layer_idx]\n",
    "                    pooled = hidden.mean(dim=1).cpu()  # [batch, hidden_dim]\n",
    "                    latent_states[f'layer_{layer_idx}'] = pooled\n",
    "            \n",
    "            # Save\n",
    "            torch.save(latent_states, cache_path)\n",
    "            \n",
    "            # Free memory\n",
    "            del outputs, inputs\n",
    "            if idx % 100 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"‚úÖ Latent extraction complete! ({elapsed/60:.1f} minutes)\")\n",
    "\n",
    "# ===== EXTRACTION LOGIC =====\n",
    "\n",
    "# Check if cache already exists\n",
    "existing_cache = check_cache_completeness(config.LATENT_CACHE_DIR, len(train_data))\n",
    "EXTRACT_LATENTS = existing_cache < len(train_data)\n",
    "\n",
    "if EXTRACT_LATENTS:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üß† TEACHER KNOWLEDGE EXTRACTION\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Cache status: {existing_cache}/{len(train_data)} files exist\")\n",
    "    print(f\"Will extract: {len(train_data) - existing_cache} samples\")\n",
    "    \n",
    "    if config.IS_COLAB and not config.USE_GDRIVE:\n",
    "        print(f\"\\n‚ö†Ô∏è  WARNING: Google Drive not mounted!\")\n",
    "        print(f\"   Cache will be lost when session ends.\")\n",
    "        print(f\"   Recommendation: Restart and mount Drive first.\")\n",
    "        proceed = input(\"Continue anyway? (yes/no): \")\n",
    "        if proceed.lower() != 'yes':\n",
    "            raise Exception(\"Stopped by user. Please mount Google Drive and restart.\")\n",
    "    \n",
    "    teacher_model, teacher_tokenizer = load_teacher_model()\n",
    "    extract_latent_states(\n",
    "        teacher_model, \n",
    "        teacher_tokenizer, \n",
    "        train_data, \n",
    "        config.LATENT_CACHE_DIR\n",
    "    )\n",
    "    \n",
    "    # Free teacher model\n",
    "    del teacher_model, teacher_tokenizer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"üóëÔ∏è  Teacher model freed from memory\")\n",
    "else:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"‚úÖ CACHE FOUND - Skipping teacher extraction\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Using existing cache: {config.LATENT_CACHE_DIR}\")\n",
    "    print(f\"Files: {existing_cache}/{len(train_data)}\")\n",
    "    print(f\"This saves ~{existing_cache * 2.5 / 60:.1f} minutes! üöÄ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b02616d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_student_model():\n",
    "    \"\"\"Load student model with LoRA\"\"\"\n",
    "    print(\"üéì Loading Student Model with LoRA...\")\n",
    "    \n",
    "    # Load base model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        config.STUDENT_MODEL,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.STUDENT_MODEL)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # LoRA config\n",
    "    lora_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        r=config.LORA_R,\n",
    "        lora_alpha=config.LORA_ALPHA,\n",
    "        lora_dropout=config.LORA_DROPOUT,\n",
    "        target_modules=config.LORA_TARGET_MODULES,\n",
    "        bias=\"none\"\n",
    "    )\n",
    "    \n",
    "    # Apply LoRA\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "student_model, student_tokenizer = setup_student_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00c7339",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistillationTrainer(Trainer):\n",
    "    \"\"\"Custom trainer with latent distillation loss\"\"\"\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # Get student outputs with hidden states\n",
    "        outputs = model(\n",
    "            input_ids=inputs['input_ids'],\n",
    "            attention_mask=inputs['attention_mask'],\n",
    "            labels=inputs['input_ids'],\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        # 1. Output loss (standard language modeling)\n",
    "        loss_output = outputs.loss\n",
    "        \n",
    "        # 2. Latent distillation loss\n",
    "        loss_latent = 0.0\n",
    "        if 'teacher_latents' in inputs:\n",
    "            teacher_latents = inputs['teacher_latents']\n",
    "            student_hidden = outputs.hidden_states\n",
    "            \n",
    "            num_latent_layers = 0\n",
    "            for layer_idx in config.LATENT_LAYERS:\n",
    "                layer_key = f'layer_{layer_idx}'\n",
    "                if layer_key in teacher_latents and layer_idx < len(student_hidden):\n",
    "                    # Get student hidden at same layer\n",
    "                    student_h = student_hidden[layer_idx]\n",
    "                    student_pooled = student_h.mean(dim=1)  # [batch, hidden]\n",
    "                    \n",
    "                    # Teacher latent\n",
    "                    teacher_h = teacher_latents[layer_key].to(student_pooled.device)\n",
    "                    \n",
    "                    # MSE loss\n",
    "                    loss_latent += F.mse_loss(student_pooled, teacher_h)\n",
    "                    num_latent_layers += 1\n",
    "            \n",
    "            if num_latent_layers > 0:\n",
    "                loss_latent /= num_latent_layers\n",
    "        \n",
    "        # Combined loss\n",
    "        total_loss = (config.ALPHA_OUTPUT * loss_output + \n",
    "                      config.BETA_LATENT * loss_latent)\n",
    "        \n",
    "        return (total_loss, outputs) if return_outputs else total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e9e886",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ReasoningDataset(\n",
    "    train_data,\n",
    "    student_tokenizer,\n",
    "    max_length=config.MAX_LENGTH,\n",
    "    latent_dir=config.LATENT_CACHE_DIR\n",
    ")\n",
    "\n",
    "test_dataset = ReasoningDataset(\n",
    "    test_data,\n",
    "    student_tokenizer,\n",
    "    max_length=config.MAX_LENGTH,\n",
    "    latent_dir=None  # No latent for test\n",
    ")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=config.OUTPUT_DIR,\n",
    "    num_train_epochs=config.NUM_EPOCHS,\n",
    "    per_device_train_batch_size=config.BATCH_SIZE,\n",
    "    per_device_eval_batch_size=config.BATCH_SIZE,\n",
    "    gradient_accumulation_steps=config.GRADIENT_ACCUM,\n",
    "    learning_rate=config.LEARNING_RATE,\n",
    "    warmup_steps=config.WARMUP_STEPS,\n",
    "    logging_steps=50,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=True,\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = DistillationTrainer(\n",
    "    model=student_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "print(\"üöÄ Training configuration ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655bad9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üî• Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "# Save final model\n",
    "trainer.save_model(f\"{config.OUTPUT_DIR}/final_model\")\n",
    "student_tokenizer.save_pretrained(f\"{config.OUTPUT_DIR}/final_model\")\n",
    "\n",
    "print(\"‚úÖ Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276e9e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_reasoning(model, tokenizer, test_data, num_samples=50):\n",
    "    \"\"\"Evaluate reasoning accuracy\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    print(\"üìä Evaluating reasoning accuracy...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx in tqdm(range(min(num_samples, len(test_data)))):\n",
    "            item = test_data[idx]\n",
    "            prompt = prepare_prompt(item['question'])\n",
    "            \n",
    "            inputs = tokenizer(\n",
    "                prompt,\n",
    "                return_tensors='pt',\n",
    "                truncation=True,\n",
    "                max_length=256\n",
    "            ).to(model.device)\n",
    "            \n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=128,\n",
    "                temperature=0.7,\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "            \n",
    "            generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            \n",
    "            # Simple accuracy check (contains answer)\n",
    "            ground_truth = str(item['answer'])\n",
    "            if ground_truth in generated:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "            \n",
    "            # Print first 3 examples\n",
    "            if idx < 3:\n",
    "                print(f\"\\n{'='*60}\")\n",
    "                print(f\"Q: {item['question']}\")\n",
    "                print(f\"Ground Truth: {ground_truth}\")\n",
    "                print(f\"Generated: {generated[len(prompt):][:200]}...\")\n",
    "    \n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    print(f\"\\n‚úÖ Accuracy: {accuracy:.2%} ({correct}/{total})\")\n",
    "    return accuracy\n",
    "\n",
    "# Evaluate\n",
    "accuracy = evaluate_reasoning(student_model, student_tokenizer, test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94079c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metrics\n",
    "results = {\n",
    "    'accuracy': float(accuracy),\n",
    "    'config': {\n",
    "        'teacher': config.TEACHER_MODEL,\n",
    "        'student': config.STUDENT_MODEL,\n",
    "        'lora_r': config.LORA_R,\n",
    "        'alpha_output': config.ALPHA_OUTPUT,\n",
    "        'beta_latent': config.BETA_LATENT\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f\"{config.OUTPUT_DIR}/results.json\", 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"üìÅ Results saved!\")\n",
    "\n",
    "# Inference example\n",
    "def inference(question: str):\n",
    "    \"\"\"Single inference\"\"\"\n",
    "    prompt = prepare_prompt(question)\n",
    "    inputs = student_tokenizer(prompt, return_tensors='pt').to(student_model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = student_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=200,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=student_tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    result = student_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return result[len(prompt):]\n",
    "\n",
    "# Test inference\n",
    "test_question = \"If John has 5 apples and gives 2 to Mary, how many does he have left?\"\n",
    "print(f\"\\nüß™ Test Inference:\")\n",
    "print(f\"Q: {test_question}\")\n",
    "print(f\"A: {inference(test_question)}\")\n",
    "\n",
    "print(\"\\n‚ú® Pipeline complete! Model saved at:\", config.OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcff2238",
   "metadata": {},
   "source": [
    "## üì• Download Results from Google Colab\n",
    "\n",
    "Run the cell below to download your trained model and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c792bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ Package and download model (for Colab)\n",
    "\n",
    "if config.IS_COLAB:\n",
    "    from google.colab import files\n",
    "    import shutil\n",
    "    import zipfile\n",
    "    \n",
    "    print(\"üì¶ Packaging model for download...\")\n",
    "    \n",
    "    # Create zip file\n",
    "    zip_path = \"/content/distill_model.zip\"\n",
    "    \n",
    "    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        # Add model files\n",
    "        model_dir = f\"{config.OUTPUT_DIR}/final_model\"\n",
    "        if os.path.exists(model_dir):\n",
    "            for root, dirs, files_list in os.walk(model_dir):\n",
    "                for file in files_list:\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    arcname = os.path.relpath(file_path, config.OUTPUT_DIR)\n",
    "                    zipf.write(file_path, arcname)\n",
    "        \n",
    "        # Add results\n",
    "        results_path = f\"{config.OUTPUT_DIR}/results.json\"\n",
    "        if os.path.exists(results_path):\n",
    "            zipf.write(results_path, \"results.json\")\n",
    "    \n",
    "    file_size_mb = os.path.getsize(zip_path) / (1024 * 1024)\n",
    "    print(f\"‚úÖ Package created: {file_size_mb:.1f} MB\")\n",
    "    \n",
    "    # Download\n",
    "    print(f\"‚¨áÔ∏è  Downloading...\")\n",
    "    files.download(zip_path)\n",
    "    print(f\"‚úÖ Download complete!\")\n",
    "    \n",
    "    if config.USE_GDRIVE:\n",
    "        print(f\"\\nüíæ Model also saved to Google Drive:\")\n",
    "        print(f\"   {config.OUTPUT_DIR}/final_model/\")\n",
    "        print(f\"   You can access it anytime from Drive\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  Not running on Colab - files saved locally\")\n",
    "    print(f\"   Location: {config.OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff18ca08",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üí° Google Colab Tips & Best Practices\n",
    "\n",
    "### ‚úÖ Before Starting:\n",
    "1. **Enable GPU**: Runtime ‚Üí Change runtime type ‚Üí GPU (T4 recommended)\n",
    "2. **Mount Google Drive**: Saves cache permanently\n",
    "3. **Check GPU**: Run `!nvidia-smi` to verify\n",
    "\n",
    "### ‚ö° Session Management:\n",
    "- **Colab Free**: ~12 hours session limit\n",
    "- **First run**: Extract cache (~30-60 min), saved to Drive\n",
    "- **Subsequent runs**: Load from Drive cache (instant!)\n",
    "- **Tip**: Keep tab active to prevent disconnection\n",
    "\n",
    "### üíæ Storage Strategy:\n",
    "\n",
    "| What | Where | Why |\n",
    "|------|-------|-----|\n",
    "| **Teacher Cache** | Google Drive | Reuse forever (2.5GB) |\n",
    "| **Student Model** | Google Drive | Access later (~100MB LoRA) |\n",
    "| **Training Logs** | /content/ | Temporary, download if needed |\n",
    "\n",
    "### üîÑ If Session Disconnects:\n",
    "\n",
    "```python\n",
    "# Cache is safe in Google Drive!\n",
    "# Just re-run from training cell:\n",
    "# - Mounts Drive\n",
    "# - Detects existing cache\n",
    "# - Continues training\n",
    "```\n",
    "\n",
    "### üìä Monitor Training:\n",
    "\n",
    "```python\n",
    "# Check GPU usage\n",
    "!nvidia-smi\n",
    "\n",
    "# Check file sizes\n",
    "!du -sh /content/drive/MyDrive/latent_cache\n",
    "!du -sh /content/drive/MyDrive/distill_output\n",
    "```\n",
    "\n",
    "### ‚ö†Ô∏è Common Issues:\n",
    "\n",
    "**GPU not available:**\n",
    "```\n",
    "Runtime ‚Üí Change runtime type ‚Üí GPU ‚Üí Save\n",
    "Then Restart runtime\n",
    "```\n",
    "\n",
    "**Drive quota exceeded:**\n",
    "```python\n",
    "# Use /content/ instead (temporary)\n",
    "config.OUTPUT_DIR = \"/content/distill_output\"\n",
    "config.LATENT_CACHE_DIR = \"/content/latent_cache\"\n",
    "```\n",
    "\n",
    "**Session disconnected:**\n",
    "```\n",
    "- Cache is safe in Drive\n",
    "- Just re-run cells\n",
    "- Training resumes from last checkpoint\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üéì What This Notebook Does:\n",
    "\n",
    "1. **Load Models**: Teacher (Llama-2-13B) + Student (Mistral-7B)\n",
    "2. **Extract Knowledge**: Teacher's hidden states (cached)\n",
    "3. **Train Student**: With LoRA + Knowledge Distillation\n",
    "4. **Evaluate**: On GSM8K reasoning tasks\n",
    "5. **Save**: Model to Drive + Download option\n",
    "\n",
    "**Total Time:**\n",
    "- First run: ~90 min (with extraction)\n",
    "- Cached run: ~30 min (skip extraction)\n",
    "\n",
    "---\n",
    "\n",
    "**Happy Training on Colab! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
