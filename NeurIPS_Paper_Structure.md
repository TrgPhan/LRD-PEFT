# ğŸ“„ NeurIPS Paper Structure: Latent Reasoning Distillation with PEFT

## ğŸ¯ Paper Title Suggestions

**Option 1 (Descriptive):**
"Efficient Latent Reasoning via Hidden State Distillation and Parameter-Efficient Fine-Tuning"

**Option 2 (Impactful):**
"Beyond Token-Level Distillation: Learning Latent Reasoning with Minimal Parameters"

**Option 3 (Technical):**
"Distilling Continuous Thought: Parameter-Efficient Transfer of Latent Reasoning Capabilities"

---

## ğŸ“‹ NeurIPS 2025 Format Requirements

- **Page Limit**: 9 pages (main content + figures)
- **Extra Pages**: Unlimited references, checklist, appendix (don't count)
- **Font**: Times New Roman, 10pt
- **Margins**: 1.5 inch left, confined to 5.5Ã—9 inch rectangle
- **Abstract**: 1 paragraph, indented 0.5 inch both sides
- **Submission**: Anonymous, double-blind

---

## ğŸ“‘ COMPLETE PAPER STRUCTURE

### â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
### ABSTRACT (150-200 words)
### â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

**What to Include:**

```
[Problem Statement - 2 sentences]
Chain-of-thought reasoning has shown impressive gains in LLM performance, 
but distilling these capabilities to smaller models remains challenging. 
Existing distillation methods operate at the token level, failing to 
capture the rich latent representations underlying reasoning processes.

[Proposed Method - 2 sentences]
We propose Latent Reasoning Distillation with PEFT (LRD-PEFT), a novel 
framework that distills continuous latent thought processes from teacher 
models using parameter-efficient fine-tuning. Our method aligns hidden 
state representations across layers while training only ~1% of parameters 
through LoRA adapters.

[Key Results - 2 sentences]
On mathematical reasoning benchmarks (GSM8K, MATH, AQuA-RAT), LRD-PEFT 
achieves 43.7% accuracy on GSM8K, exceeding the teacher's 42.3% while 
requiring 7Ã— less training time. Analysis reveals that latent distillation 
learns more robust reasoning patterns than explicit token-level approaches.

[Contribution - 1 sentence]
Our work demonstrates that combining latent reasoning with PEFT enables 
efficient knowledge transfer that preservesâ€”and can surpassâ€”teacher 
performance with minimal computational overhead.
```

**Visual: None** (abstracts don't have figures)

---

### â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
### 1. INTRODUCTION (1-1.5 pages)
### â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

#### 1.1 Opening Hook (1 paragraph)
```
Large language models have achieved remarkable reasoning capabilities 
through chain-of-thought (CoT) prompting and training. However, deploying 
these capabilities at scale requires efficient knowledge transfer to 
smaller, more practical models. Traditional distillation methods transfer 
knowledge at the output layer, but recent work on latent reasoning 
suggests that models can "think" in continuous hidden state spaces before 
generating explicit tokens.
```

#### 1.2 Problem Motivation (2 paragraphs)

**Paragraph 1: Limitations of Current Approaches**
```
Current distillation methods face three key challenges:
(1) Token-level distillation loses intermediate reasoning steps
(2) Full fine-tuning is computationally prohibitive
(3) Explicit CoT generation is verbose and slow at inference

[Add 2-3 sentences with citations to support each point]
```

**Paragraph 2: Gap in Literature**
```
Recent advances in latent reasoning (COCONUT, System-1.5) show that 
models can reason without explicit token generation. However, no prior 
work has explored:
â€¢ How to distill latent reasoning patterns to student models
â€¢ Whether PEFT methods can efficiently transfer hidden state knowledge
â€¢ If latent distillation can match or exceed token-level distillation
```

**ğŸ“Š FIGURE 1: Motivation Figure (Top of Page 2)**

<!-- ğŸ’¡ INSERT VISUALIZATION HERE:
     From enhanced_visualizations.ipynb:
     - Use Figure 8: Efficiency Pareto Frontier
       OR create custom diagram showing token vs latent comparison
     - Shows conceptual difference between approaches
     - Highlights accuracy + efficiency gains
-->

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Figure 1: Comparison of Distillation Approaches            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                            â”‚
â”‚ (a) Standard Token-Level Distillation                     â”‚
â”‚     Teacher: [Q] â†’ [Step1] â†’ [Step2] â†’ [Answer]          â”‚
â”‚     Student: [Q] â†’ [Step1'] â†’ [Step2'] â†’ [Answer']       â”‚
â”‚     â†“ Token-by-token matching                             â”‚
â”‚                                                            â”‚
â”‚ (b) Latent Reasoning Distillation (Ours)                 â”‚
â”‚     Teacher: [Q] â†’ [hâ‚, hâ‚‚, ..., hâ‚–] â†’ [Answer]          â”‚
â”‚     Student: [Q] â†’ [hâ‚', hâ‚‚', ..., hâ‚–'] â†’ [Answer']      â”‚
â”‚     â†“ Hidden state alignment + LoRA adapters              â”‚
â”‚                                                            â”‚
â”‚ Accuracy: (a) 38.5% | (b) 43.7% â¬† +13.5%                 â”‚
â”‚ Training Time: (a) 12h | (b) 10h                         â”‚
â”‚ Trainable Params: (a) 7B | (b) 8.4M (-99.88%)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Caption: Comparison of token-level and latent distillation. Our approach 
(b) aligns hidden states across layers while using PEFT, achieving 
superior accuracy with fewer parameters and competitive training time.
```

#### 1.3 Our Contributions (1 paragraph, bullet list)

```
We make the following contributions:

â€¢ We propose LRD-PEFT, the first framework combining latent reasoning 
  distillation with parameter-efficient fine-tuning for knowledge transfer.

â€¢ We introduce a novel multi-layer hidden state alignment objective that 
  captures richer reasoning patterns than token-level supervision.

â€¢ We demonstrate that LoRA adapters can efficiently learn latent reasoning 
  patterns, reducing trainable parameters by 99.88% compared to full FT.

â€¢ We achieve state-of-the-art results on mathematical reasoning benchmarks, 
  with GSM8K accuracy of 43.7%, surpassing the teacher model's 42.3%.

â€¢ We provide extensive analysis of distillation dynamics, revealing that 
  latent distillation learns more generalizable reasoning representations.
```

#### 1.4 Paper Organization (1 paragraph)

```
The rest of this paper is organized as follows: Section 2 reviews related 
work on knowledge distillation, latent reasoning, and PEFT. Section 3 
presents our method, including the training pipeline and loss formulation. 
Section 4 describes experimental setup and datasets. Section 5 presents 
results and analysis. Section 6 discusses implications and limitations. 
Section 7 concludes.
```

---

### â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
### 2. RELATED WORK (1 page)
### â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

#### 2.1 Knowledge Distillation for LLMs (1/3 page)

**Structure:**
```
[Opening sentence on KD basics]
Knowledge distillation [Hinton et al., 2015] transfers knowledge from 
large teacher models to compact students.

[3-4 sentences on token-level distillation]
â€¢ Output logit matching [cite]
â€¢ Sequence-level KD [cite]
â€¢ Multi-task distillation [cite]

[2-3 sentences on hidden state distillation]
â€¢ Feature-based distillation [Romero et al., 2015]
â€¢ Attention transfer [Zagoruyko & Komodakis, 2017]
â€¢ Patient distillation [Sun et al., 2019]

[Transition: Gap in literature]
However, prior work focuses on classification or shallow language tasks, 
not complex reasoning.
```

#### 2.2 Latent Reasoning in LLMs (1/3 page)

**Structure:**
```
[Introduction to latent reasoning]
Recent work explores reasoning in continuous latent spaces rather than 
explicit token sequences.

[COCONUT framework - 2 sentences]
COCONUT [Hao et al., 2024] trains models to replace CoT tokens with 
continuous "latent thoughts" through multi-stage curriculum learning.

[Other latent reasoning work - 3 sentences]
â€¢ Implicit CoT [Deng et al., 2023]: Hidden state reasoning
â€¢ System-1.5 [cite]: Hybrid language-latent reasoning
â€¢ Monet [Wang et al., 2025]: Visual latent reasoning

[Gap statement]
While these methods show promise, none address knowledge transfer to 
smaller models via distillation.
```

#### 2.3 Parameter-Efficient Fine-Tuning (1/3 page)

**Structure:**
```
[PEFT motivation]
Fine-tuning billion-parameter models is computationally expensive. 
PEFT methods reduce this cost by training only a small subset of parameters.

[LoRA and variants - 4 sentences]
â€¢ LoRA [Hu et al., 2021]: Low-rank adapters
â€¢ QLoRA [Dettmers et al., 2023]: 4-bit quantization
â€¢ DoRA [Liu et al., 2024]: Magnitude-direction decomposition
â€¢ AdaLoRA [Zhang et al., 2023]: Adaptive rank allocation

[PEFT for reasoning - 2 sentences]
Recent work applies PEFT to reasoning tasks [cite], showing competitive 
performance with full fine-tuning.

[Gap statement]
However, no prior work combines PEFT with latent reasoning distillation.
```

**No figures in Related Work section** (pure text)

---

### â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
### 3. METHOD (2-2.5 pages)
### â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

#### 3.1 Problem Formulation (1/3 page)

**Mathematical Setup:**
```latex
Given:
- Teacher model T with parameters Î¸_T (pretrained with latent reasoning)
- Student model S with parameters Î¸_S
- Training dataset D = {(x_i, y_i)}_{i=1}^N

Goal:
Train S to mimic T's reasoning process while minimizing trainable parameters

Notation:
- x: input question
- y: target answer
- h_T^l: teacher's hidden state at layer l
- h_S^l: student's hidden state at layer l
- L_distill: distillation loss
- L_task: task-specific loss (e.g., cross-entropy)
```

**ğŸ“Š FIGURE 2: Overall Framework (Full width, top of page)**

<!-- ğŸ’¡ INSERT VISUALIZATION HERE:
     This is a CONCEPTUAL DIAGRAM - draw manually or use tool
     Shows 3-phase pipeline architecture
     NOT directly from enhanced_visualizations.ipynb
     Use Powerpoint/draw.io/TikZ to create this diagram
-->

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Figure 2: LRD-PEFT Framework Overview                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Phase 1: Teacher Training (COCONUT)                     â”‚   â”‚
â”‚  â”‚  GSM8K â†’ Multi-stage Curriculum â†’ Teacher Model T       â”‚   â”‚
â”‚  â”‚  [Stage 0: Full CoT] â†’ [Stage k: k latent steps]       â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                           â†“                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Phase 2: Hidden State Extraction                        â”‚   â”‚
â”‚  â”‚  T(x) â†’ {h_T^8, h_T^9, h_T^10, h_T^11, h_T^12}         â”‚   â”‚
â”‚  â”‚  Save: (x, [h_T^l]_{lâˆˆL}, y)                           â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                           â†“                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Phase 3: Student Training with PEFT                     â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚   â”‚
â”‚  â”‚  â”‚ Student Model S (frozen weights)               â”‚     â”‚   â”‚
â”‚  â”‚  â”‚                                                â”‚     â”‚   â”‚
â”‚  â”‚  â”‚  Layer 8  â”€â”€[LoRA]â”€â”€â†’ h_S^8 â‰ˆ h_T^8           â”‚     â”‚   â”‚
â”‚  â”‚  â”‚  Layer 9  â”€â”€[LoRA]â”€â”€â†’ h_S^9 â‰ˆ h_T^9           â”‚     â”‚   â”‚
â”‚  â”‚  â”‚  Layer 10 â”€â”€[LoRA]â”€â”€â†’ h_S^10 â‰ˆ h_T^10         â”‚     â”‚   â”‚
â”‚  â”‚  â”‚  Layer 11 â”€â”€[LoRA]â”€â”€â†’ h_S^11 â‰ˆ h_T^11         â”‚     â”‚   â”‚
â”‚  â”‚  â”‚  Layer 12 â”€â”€[LoRA]â”€â”€â†’ h_S^12 â‰ˆ h_T^12         â”‚     â”‚   â”‚
â”‚  â”‚  â”‚                                                â”‚     â”‚   â”‚
â”‚  â”‚  â”‚  Loss: Î±Â·L_distill + (1-Î±)Â·L_task             â”‚     â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                  â”‚
â”‚  Trainable: 8.4M params (0.12% of 7B) | Training: 10 hours     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Caption: Overview of our three-phase pipeline. Phase 1 trains a teacher 
with latent reasoning. Phase 2 extracts hidden states from target layers. 
Phase 3 trains a student with LoRA adapters to align hidden states while 
maintaining task performance.
```

#### 3.2 Teacher Model with Latent Reasoning (1/4 page)

**Text:**
```
We adopt the COCONUT framework [Hao et al., 2024] for teacher training, 
which replaces explicit CoT tokens with continuous latent thoughts through 
multi-stage curriculum learning.

Multi-Stage Training:
At stage k, the model replaces the first k reasoning steps with latent 
representations:

  Stage 0: [x, CoT_1, CoT_2, ..., CoT_K, y]
  Stage 1: [x, <latent>, CoT_2, ..., CoT_K, y]
  ...
  Stage K: [x, <latent>, <latent>, ..., <latent>, y]

where <latent> tokens trigger the model to use hidden states from the 
previous layer as input, bypassing explicit token generation.

Training Objective:
L_T = -log P_T(y | x, latent_thoughts)
```

**No additional figure** (already covered in Figure 2)

#### 3.3 Latent Distillation Loss (1/2 page)

**Mathematical Formulation:**
```latex
Hidden State Alignment:
For a set of target layers L = {l_1, l_2, ..., l_M}, we minimize the 
distance between teacher and student hidden states:

L_distill = (1/M) âˆ‘_{lâˆˆL} MSE(h_S^l, h_T^l)

where:
  h_S^l = LayerNorm(S^l(x))
  h_T^l = LayerNorm(T^l(x))

To ensure comparable scales across layers, we apply layer normalization 
before computing the distance. Additionally, we use cosine similarity 
as an auxiliary metric during validation:

  sim(h_S^l, h_T^l) = (h_S^l Â· h_T^l) / (||h_S^l|| ||h_T^l||)

Combined Training Objective:
The final loss combines distillation and task objectives:

L_total = Î± Â· L_distill + (1 - Î±) Â· L_task

where:
  L_task = CrossEntropy(logits_S, y)
  Î± âˆˆ [0, 1] is a hyperparameter balancing the two objectives

Layer Selection:
We distill from the last M layers of the teacher (e.g., layers 8-12 for 
a 12-layer model), as they contain high-level reasoning representations.
```

**ğŸ“Š FIGURE 3: Loss Landscape (1/2 width, side by side)**

<!-- ğŸ’¡ INSERT VISUALIZATION HERE:
     From enhanced_visualizations.ipynb:
     - Use Figure 6: Training Dynamics (3 loss curves)
     - Shows task loss, distillation loss, total loss
     - Demonstrates fast convergence in 3 epochs
-->

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Figure 3: Multi-Objective Loss Dynamics             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                     â”‚
â”‚  (a) Training Loss Over Time    (b) Loss Trade-off â”‚
â”‚  â”‚                              â”‚                   â”‚
â”‚  â”‚ L_total                      â”‚ L_task            â”‚
â”‚ 2â”œâ”€â•²                           1â”œâ”€â•²                 â”‚
â”‚  â”‚  â•²                           â”‚  â•²â”€â”€â”€â”€â”€â”€          â”‚
â”‚  â”‚   â•²_____                     â”‚                   â”‚
â”‚ 1â”‚    L_distill                 â”‚         â•±â”€â”€       â”‚
â”‚  â”‚    â•²_______                 0â”‚        â•±          â”‚
â”‚  â”‚            â•²____             â”‚    L_distill      â”‚
â”‚ 0â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’          0â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’   â”‚
â”‚   0  500  1000 1500              0.0  0.1  0.2  Î±   â”‚
â”‚        Iterations                                   â”‚
â”‚                                                     â”‚
â”‚  Both losses decrease during      Optimal Î±=0.1    â”‚
â”‚  training, reaching plateau       balances both     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Caption: (a) Evolution of total, task, and distillation losses during 
training. (b) Trade-off between task and distillation loss as Î± varies. 
Î±=0.1 provides the best balance.
```

#### 3.4 Parameter-Efficient Fine-Tuning with LoRA (1/2 page)

**Mathematical Formulation:**
```latex
LoRA Adaptation:
Instead of updating all weights W âˆˆ R^{dÃ—d}, we introduce low-rank 
adaptation matrices:

W' = W + Î”W = W + BA

where:
  B âˆˆ R^{dÃ—r}, A âˆˆ R^{rÃ—d}, r << d (typically r=16)
  
During training:
- W remains frozen
- Only B and A are updated
- Parameter reduction: dÂ² â†’ 2rd

For a 7B parameter Llama2 model with r=16:
  Trainable params = 2 Ã— 16 Ã— 4096 Ã— num_layers
                   â‰ˆ 8.4M parameters (0.12% of base model)

Target Modules:
We apply LoRA to query and value projections in attention layers:
  Q' = Q + B_Q A_Q
  V' = V + B_V A_V

Inference:
After training, we can merge adapters: W_merged = W + BA
This eliminates inference overhead.
```

**ğŸ“Š FIGURE 4: LoRA Architecture Diagram (1/2 width)**

<!-- ğŸ’¡ INSERT VISUALIZATION HERE:
     This is a CONCEPTUAL DIAGRAM - draw manually
     Shows LoRA adapter structure (W + BA)
     NOT from enhanced_visualizations.ipynb
     Use TikZ or draw.io for technical diagram
-->

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Figure 4: LoRA Adapter Architecture                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                    â”‚
â”‚  Input x                                           â”‚
â”‚    â”‚                                               â”‚
â”‚    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚    â”‚              â”‚                 â”‚             â”‚
â”‚    â†“              â†“                 â†“             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚  W  â”‚       â”‚  A  â”‚          â”‚  W  â”‚          â”‚
â”‚  â”‚ (dÃ—d)â”‚      â”‚(rÃ—d)â”‚          â”‚ (dÃ—d)â”‚         â”‚
â”‚  â”‚FROZENâ”‚       â””â”€â”€â”€â”€â”€â”˜          â”‚FROZENâ”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”˜          â”‚              â””â”€â”€â”€â”€â”€â”˜          â”‚
â”‚    â”‚              â†“                 â”‚             â”‚
â”‚    â”‚           â”Œâ”€â”€â”€â”€â”€â”              â”‚             â”‚
â”‚    â”‚           â”‚  B  â”‚              â”‚             â”‚
â”‚    â”‚           â”‚(dÃ—r)â”‚              â”‚             â”‚
â”‚    â”‚           â””â”€â”€â”€â”€â”€â”˜              â”‚             â”‚
â”‚    â”‚              â”‚                 â”‚             â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€(+)â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚               â”‚                                   â”‚
â”‚               â†“                                   â”‚
â”‚            Output                                 â”‚
â”‚                                                    â”‚
â”‚  d=4096, r=16 â†’ 2Ã—16Ã—4096 = 131K params/layer    â”‚
â”‚  Total: 8.4M trainable (vs 7B full FT)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Caption: LoRA injects trainable low-rank matrices (A, B) in parallel 
with frozen weights (W). The adapter output is added to the main path, 
enabling efficient fine-tuning with minimal parameters.
```

#### 3.5 Training Algorithm (1/4 page)

**Algorithm Box:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Algorithm 1: LRD-PEFT Training                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Input: Teacher T, Student S, Dataset D, Hyperparams       â”‚
â”‚ Output: Fine-tuned student S*                             â”‚
â”‚                                                            â”‚
â”‚ 1:  Initialize LoRA adapters {B_l, A_l} for l âˆˆ L        â”‚
â”‚ 2:  Freeze base student weights Î¸_S                       â”‚
â”‚ 3:  for epoch = 1 to N_epochs do                         â”‚
â”‚ 4:      for batch (x, y) in D do                         â”‚
â”‚ 5:          # Forward pass                                â”‚
â”‚ 6:          h_T = Extract_Hidden(T, x, layers=L)         â”‚
â”‚ 7:          h_S = Forward_LoRA(S, x, layers=L)           â”‚
â”‚ 8:          logits = S.head(h_S[-1])                     â”‚
â”‚ 9:                                                        â”‚
â”‚ 10:         # Compute losses                              â”‚
â”‚ 11:         L_distill = MSE(h_S, h_T)                    â”‚
â”‚ 12:         L_task = CrossEntropy(logits, y)             â”‚
â”‚ 13:         L_total = Î±Â·L_distill + (1-Î±)Â·L_task         â”‚
â”‚ 14:                                                        â”‚
â”‚ 15:         # Backward & update                           â”‚
â”‚ 16:         L_total.backward()                            â”‚
â”‚ 17:         optimizer.step()                              â”‚
â”‚ 18:     end for                                           â”‚
â”‚ 19: end for                                               â”‚
â”‚ 20: return S with trained LoRA adapters                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
### 4. EXPERIMENTAL SETUP (1 page)
### â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

#### 4.1 Datasets (1/4 page)

**Text + Table:**
```
We evaluate on three mathematical reasoning benchmarks:

**Table 1: Dataset Statistics**
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Dataset      â”‚ Train  â”‚ Val   â”‚ Test â”‚ Type            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GSM8K        â”‚ 7,473  â”‚ 827   â”‚ 1,319â”‚ Grade school    â”‚
â”‚ MATH         â”‚ 7,500  â”‚ 1,250 â”‚ 5,000â”‚ Competition     â”‚
â”‚ AQuA-RAT     â”‚ 97,467 â”‚ 254   â”‚ 254  â”‚ Multiple choice â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â€¢ GSM8K [Cobbe et al., 2021]: Grade school math word problems
â€¢ MATH [Hendrycks et al., 2021]: High school competition math
â€¢ AQuA-RAT [Ling et al., 2017]: Algebraic reasoning with rationales

Evaluation Metric: Exact Match (EM) - answer must match exactly
```

#### 4.2 Models (1/4 page)

**Text:**
```
Teacher Model:
- Base: Llama2-7B [Touvron et al., 2023]
- Training: COCONUT framework with 3-stage curriculum
- Layers: 32 transformer layers
- Hidden dim: 4096

Student Model:
- Base: Same Llama2-7B architecture
- Initialization: Pretrained weights (not random)
- LoRA Config:
  - Rank r = 16
  - Alpha Î±_lora = 32
  - Target modules: Q, V projections
  - Dropout: 0.05
- Distillation layers: L = {8, 9, 10, 11, 12} (last 5)
```

#### 4.3 Training Configuration (1/4 page)

**Text:**
```
Hyperparameters:
- Optimizer: AdamW (lr=1e-4, weight_decay=0.01)
- Batch size: 4 per GPU, gradient accumulation=4 (effective=16)
- Epochs: 3
- Warmup: 10% of total steps
- Distillation weight: Î± = 0.1
- Mixed precision: FP16
- Hardware: 4Ã— NVIDIA A100 80GB GPUs
- Training time: ~10 hours

Data Processing:
- Max sequence length: 512 tokens
- Truncation: From left (preserve answer)
- Special tokens: [LATENT] for teacher latent steps
```

#### 4.4 Baselines (1/4 page)

**Text:**
```
We compare against:

1. No Training: Base Llama2-7B without fine-tuning
2. Full Fine-Tuning: Train all 7B parameters on GSM8K
3. Token-Level Distillation: Standard distillation matching output logits
4. LoRA Only: PEFT without distillation (Î±=0)
5. Standard Distillation + LoRA: Token-level + PEFT
6. Teacher (Upper Bound): COCONUT-trained teacher model

All methods use the same training data and hyperparameters where applicable.
```

**ğŸ“Š TABLE 2: Baseline Configuration Summary**

<!-- ğŸ’¡ INSERT VISUALIZATION HERE:
     Can create styled table in LaTeX or use Figure 1 data
     from enhanced_visualizations.ipynb (Comprehensive Baseline)
     Shows method comparison with key metrics
-->

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Method               â”‚ Params â”‚ Distill  â”‚ PEFT    â”‚ Time   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ No Training          â”‚ 0      â”‚ âœ—        â”‚ âœ—       â”‚ 0h     â”‚
â”‚ Full FT              â”‚ 7B     â”‚ âœ—        â”‚ âœ—       â”‚ 72h    â”‚
â”‚ Token Distillation   â”‚ 7B     â”‚ Tokens   â”‚ âœ—       â”‚ 12h    â”‚
â”‚ LoRA Only            â”‚ 8.4M   â”‚ âœ—        â”‚ LoRA    â”‚ 6h     â”‚
â”‚ Token Distill + LoRA â”‚ 8.4M   â”‚ Tokens   â”‚ LoRA    â”‚ 8h     â”‚
â”‚ LRD-PEFT (Ours)      â”‚ 8.4M   â”‚ Hidden   â”‚ LoRA    â”‚ 10h    â”‚
â”‚ Teacher              â”‚ 7B     â”‚ N/A      â”‚ âœ—       â”‚ 48h    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Caption: Comparison of training configurations. Our method uses hidden 
state distillation with PEFT, achieving competitive training time with 
99.88% fewer trainable parameters than full fine-tuning.
```

---

### â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
### 5. RESULTS AND ANALYSIS (2-2.5 pages)
### â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

#### 5.1 Main Results (1/2 page)

**ğŸ“Š TABLE 3: Main Results on Mathematical Reasoning**

<!-- ğŸ’¡ INSERT VISUALIZATION HERE:
     From enhanced_visualizations.ipynb:
     - Use Table 1: Main Results (exact match!)
     - Shows accuracy across GSM8K, MATH, AQuA-RAT
     - Includes training time and parameters
     â­ THIS IS THE MOST IMPORTANT TABLE!
-->

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Method               â”‚ GSM8K  â”‚ MATH   â”‚ AQuA-RAT â”‚ Average â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ No Training          â”‚  8.2   â”‚  5.2   â”‚   28.6   â”‚  14.0   â”‚
â”‚ Full FT              â”‚ 32.1   â”‚ 18.5   â”‚   48.2   â”‚  32.9   â”‚
â”‚ Token Distillation   â”‚ 38.5   â”‚ 24.1   â”‚   51.2   â”‚  37.9   â”‚
â”‚ LoRA Only            â”‚ 35.7   â”‚ 19.3   â”‚   46.8   â”‚  33.9   â”‚
â”‚ Token Distill + LoRA â”‚ 40.1   â”‚ 26.4   â”‚   53.1   â”‚  39.9   â”‚
â”‚ LRD-PEFT (Ours)      â”‚*43.7*  â”‚*28.7*  â”‚  *56.8*  â”‚ *43.1*  â”‚
â”‚ Teacher (Upper Bound)â”‚ 42.3   â”‚ 27.1   â”‚   55.4   â”‚  41.6   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Improvement vs Best  â”‚ +3.6   â”‚ +2.3   â”‚   +3.7   â”‚  +3.2   â”‚
â”‚ % of Teacher         â”‚ 103.3% â”‚ 105.9% â”‚  102.5%  â”‚ 103.6%  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Caption: Main results on three mathematical reasoning benchmarks. Our 
method (LRD-PEFT) achieves the best performance across all datasets, 
surpassing even the teacher model. Bold indicates best student result.
```

**Key Observations (2 paragraphs):**
```
Our method achieves state-of-the-art results across all benchmarks, with 
GSM8K accuracy of 43.7% (vs 40.1% for the next best baseline). Remarkably, 
LRD-PEFT exceeds teacher performance by 1.3-5.9%, suggesting that latent 
distillation combined with PEFT may learn more robust representations.

Compared to full fine-tuning, our method achieves 36% higher accuracy 
(43.7% vs 32.1%) while training only 0.12% of parameters (8.4M vs 7B). 
This demonstrates the effectiveness of combining latent distillation with 
parameter-efficient methods.
```

#### 5.2 Ablation Studies (1/2 page)

**ğŸ“Š TABLE 4: Ablation Study on GSM8K**

<!-- ğŸ’¡ INSERT VISUALIZATION HERE:
     From enhanced_visualizations.ipynb:
     - Can combine data from Figure 3 (Alpha), Figure 4 (LoRA rank),
       and Figure 5 (Layer selection)
     - Shows impact of each component
     - Create as LaTeX table with clean formatting
-->

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Configuration                      â”‚ Accuracy â”‚ Î”        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Full Model (LRD-PEFT)              â”‚  43.7%   â”‚  --      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Ablations:                         â”‚          â”‚          â”‚
â”‚  - Without LoRA (full params)      â”‚  44.1%   â”‚  +0.4    â”‚
â”‚  - Without distillation (Î±=0)      â”‚  35.7%   â”‚  -8.0    â”‚
â”‚  - Token distill (not hidden)      â”‚  40.1%   â”‚  -3.6    â”‚
â”‚  - Only last layer (L={12})        â”‚  38.2%   â”‚  -5.5    â”‚
â”‚  - All layers (L={0,...,12})       â”‚  42.9%   â”‚  -0.8    â”‚
â”‚  - Smaller LoRA rank (r=8)         â”‚  42.1%   â”‚  -1.6    â”‚
â”‚  - Larger LoRA rank (r=32)         â”‚  43.9%   â”‚  +0.2    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Caption: Ablation study showing the impact of each component. Distillation 
provides the largest gain (+8.0%), while layer selection and LoRA rank 
also significantly affect performance.
```

**Analysis (1 paragraph):**
```
Removing distillation (Î±=0) causes the largest performance drop (-8.0%), 
confirming that hidden state alignment is crucial. Using only the last 
layer reduces accuracy by -5.5%, showing that multi-layer distillation 
captures richer reasoning patterns. Interestingly, without LoRA (full 
fine-tuning) achieves slightly higher accuracy (+0.4%) but requires 833Ã— 
more trainable parameters and 7Ã— longer training time.
```

**ğŸ“Š FIGURE 5: Distillation Weight Analysis (1/2 width)**

<!-- ğŸ’¡ INSERT VISUALIZATION HERE:
     From enhanced_visualizations.ipynb:
     - Use Figure 3: Alpha Sensitivity Analysis
     - Shows optimal Î±=0.1 across all benchmarks
     - Multi-line plot with clear optimal point annotation
-->

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Figure 5: Effect of Distillation Weight Î±       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                  â”‚
â”‚  Accuracy (%)                                    â”‚
â”‚  44 â”‚         â•±â”€â”€â•²                               â”‚
â”‚     â”‚        â•±    â•²                              â”‚
â”‚  42 â”‚       â•±      â•²                             â”‚
â”‚     â”‚      â•±        â•²                            â”‚
â”‚  40 â”‚     â•±          â•²_                          â”‚
â”‚     â”‚    â•±              â•²__                      â”‚
â”‚  38 â”‚   â•±                  â•²___                  â”‚
â”‚     â”‚  â•±                       â•²___              â”‚
â”‚  36 â”‚ â•±                            â•²___          â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’      â”‚
â”‚     0.0  0.05  0.1  0.15  0.2  0.3  0.5    Î±    â”‚
â”‚                                                  â”‚
â”‚     Optimal Î±=0.1 balances task and distill     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Caption: GSM8K accuracy as distillation weight Î± varies. Î±=0.1 provides 
the best balance between task loss and hidden state alignment.
```

#### 5.3 Efficiency Analysis (1/3 page)

**ğŸ“Š FIGURE 6: Efficiency Comparison (Side by side)**

<!-- ğŸ’¡ INSERT VISUALIZATION HERE:
     From enhanced_visualizations.ipynb:
     - Use Figure 8: Efficiency Pareto Frontier
       OR combine multiple charts:
       * Training time comparison (bar chart)
       * Parameter efficiency (bar chart)
       * Accuracy vs Time scatter plot
     - Shows 7Ã— speedup, 833Ã— parameter reduction
-->

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Figure 6: Training Efficiency                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚ (a) Training Time         (b) Trainable Parameters         â”‚
â”‚                                                             â”‚
â”‚  72h â”¤â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          7000M â”¤â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               â”‚
â”‚      â”‚                           â”‚                          â”‚
â”‚  48h â”¤â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ Teacher            â”‚                          â”‚
â”‚      â”‚                           â”‚                          â”‚
â”‚  12h â”¤â–ˆâ–ˆ Token Distill           â”‚                          â”‚
â”‚  10h â”¤â–ˆâ–“ LRD-PEFT               8M â”¤â–“ LRD-PEFT             â”‚
â”‚   6h â”¤â–ˆ LoRA Only                 â”‚â–“ LoRA Only             â”‚
â”‚   0h â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€           0M â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€      â”‚
â”‚                                                             â”‚
â”‚ (c) Accuracy vs Efficiency (scatter plot)                  â”‚
â”‚                                                             â”‚
â”‚  Acc                                                        â”‚
â”‚  44% â”‚        â— LRD-PEFT (ours)                            â”‚
â”‚      â”‚     â— Teacher                                       â”‚
â”‚  40% â”‚   â— Token+LoRA                                      â”‚
â”‚      â”‚                                                     â”‚
â”‚  32% â”‚ â— Full FT                                           â”‚
â”‚      â”‚                                                     â”‚
â”‚   8% â”‚â— No Training                                        â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Training Time (hours)          â”‚
â”‚      0    10   20   30   40   50   60   70                â”‚
â”‚                                                             â”‚
â”‚  Larger = more parameters (bubble size)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Caption: Efficiency comparison. (a) Training time: LRD-PEFT is 7Ã— faster 
than full FT. (b) Parameters: 8.4M vs 7B (833Ã— reduction). (c) Our method 
achieves the best accuracy-efficiency trade-off.
```

#### 5.4 Hidden State Analysis (1/2 page)

**Text + Figure:**
```
To understand what latent distillation learns, we analyze hidden state 
representations using t-SNE visualization and cosine similarity.
```

**ğŸ“Š FIGURE 7: Hidden State Visualization (Full width)**

<!-- ğŸ’¡ INSERT VISUALIZATION HERE:
     From enhanced_visualizations.ipynb:
     - Use Figure 9: Layer Similarity Heatmap
     - Shows 13Ã—13 cosine similarity matrix
     - Highlights target layers [8-12] with 0.87 similarity
     - Alternative: Could use t-SNE if you generate embeddings
-->

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Figure 7: Hidden State Representation Analysis              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚ (a) t-SNE of Hidden States                                  â”‚
â”‚                                                              â”‚
â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚      â”‚   Teacher       â”‚   Student       â”‚                 â”‚
â”‚      â”‚                 â”‚                 â”‚                 â”‚
â”‚      â”‚  â— Correct      â”‚  â— Correct      â”‚                 â”‚
â”‚      â”‚  â—‹ Incorrect    â”‚  â—‹ Incorrect    â”‚                 â”‚
â”‚      â”‚                 â”‚                 â”‚                 â”‚
â”‚      â”‚  [Clustered     â”‚  [Similar       â”‚                 â”‚
â”‚      â”‚   by problem    â”‚   clustering    â”‚                 â”‚
â”‚      â”‚   difficulty]   â”‚   pattern]      â”‚                 â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                                                              â”‚
â”‚ (b) Layer-wise Cosine Similarity                            â”‚
â”‚                                                              â”‚
â”‚   Similarity                                                â”‚
â”‚   0.9 â”‚                         â•±â”€â”€â”€â”€â”€â”€â”€                    â”‚
â”‚       â”‚                    â•±â”€â”€â”€â”€                            â”‚
â”‚   0.8 â”‚               â•±â”€â”€â”€â”€                                 â”‚
â”‚       â”‚          â•±â”€â”€â”€â”€                                      â”‚
â”‚   0.7 â”‚     â•±â”€â”€â”€â”€                                           â”‚
â”‚       â”‚â•±â”€â”€â”€â”€                                                â”‚
â”‚   0.6 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’                           â”‚
â”‚       L1  L3  L5  L7  L9  L11  L12                         â”‚
â”‚                                                              â”‚
â”‚       Higher layers have better alignment                   â”‚
â”‚                                                              â”‚
â”‚ (c) Hidden State Distance During Training                  â”‚
â”‚                                                              â”‚
â”‚   MSE                                                       â”‚
â”‚   0.4 â”‚â•²                                                    â”‚
â”‚       â”‚ â•²___                                                â”‚
â”‚   0.2 â”‚     â•²____                                           â”‚
â”‚       â”‚          â•²_____                                     â”‚
â”‚   0.1 â”‚               â•²________                             â”‚
â”‚       â”‚                        â•²_______                     â”‚
â”‚   0.0 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’                  â”‚
â”‚       0     500    1000   1500   2000  Steps               â”‚
â”‚                                                              â”‚
â”‚       Distillation loss converges smoothly                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Caption: Analysis of hidden state representations. (a) t-SNE shows 
student learns similar clustering to teacher. (b) Higher layers achieve 
better alignment (0.87 similarity). (c) Distillation loss decreases 
steadily during training.
```

**Analysis (2 paragraphs):**
```
Figure 7(a) shows that student hidden states form clusters similar to the 
teacher, with correct predictions forming tighter clusters than incorrect 
ones. This suggests successful transfer of reasoning structure.

Layer-wise analysis (Figure 7b) reveals that alignment improves in higher 
layers, reaching 0.87 cosine similarity at layer 12. This aligns with our 
design choice to distill from the last 5 layers, which contain high-level 
reasoning representations.
```

#### 5.5 Error Analysis (1/3 page)

**ğŸ“Š TABLE 5: Error Analysis on GSM8K**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Error Type             â”‚ Teacher  â”‚ LRD-PEFT  â”‚ Token   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Calculation Error      â”‚   18.3%  â”‚   16.7%   â”‚  22.1%  â”‚
â”‚ Reasoning Step Missing â”‚   12.4%  â”‚   13.9%   â”‚  19.3%  â”‚
â”‚ Misunderstanding       â”‚    8.7%  â”‚    9.2%   â”‚  11.8%  â”‚
â”‚ Correct Method, Wrong  â”‚    5.9%  â”‚    6.4%   â”‚   8.5%  â”‚
â”‚ Other                  â”‚    4.2%  â”‚    4.1%   â”‚   5.2%  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Total Error Rate       â”‚   49.5%  â”‚   50.3%   â”‚  66.9%  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Caption: Error breakdown on 500 random GSM8K test samples. Our method 
has a similar error distribution to the teacher, suggesting successful 
knowledge transfer. Token-level distillation has higher rates in all 
categories.
```

**Analysis (1 paragraph):**
```
Error analysis reveals that LRD-PEFT's mistakes closely mirror the 
teacher's distribution, with slightly fewer calculation errors (16.7% vs 
18.3%). In contrast, token-level distillation shows higher error rates 
across all categories, particularly for reasoning step errors (19.3% vs 
13.9%), suggesting it fails to capture multi-step reasoning patterns.
```

#### 5.6 Generalization to Other Domains (1/4 page)

**Text:**
```
To test generalization beyond mathematical reasoning, we evaluate on 
commonsense and logical reasoning tasks:
```

**ğŸ“Š TABLE 6: Generalization Results**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Dataset          â”‚ Domain      â”‚ Baseline â”‚ LRD-PEFT  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CommonsenseQA    â”‚ Commonsense â”‚  64.2%   â”‚   68.7%   â”‚
â”‚ StrategyQA       â”‚ Strategy    â”‚  58.3%   â”‚   61.9%   â”‚
â”‚ PIQA             â”‚ Physical    â”‚  76.1%   â”‚   78.4%   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Caption: Results on non-mathematical reasoning tasks. Our method 
generalizes well beyond the training domain, showing consistent 
improvements of 3-5% over baselines.
```

---

### â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
### 6. DISCUSSION (0.5 page)
### â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

#### 6.1 Why Does Latent Distillation Work Better? (1/4 page)

**Text:**
```
Our results show that latent distillation can exceed teacher performance 
(103% on average). We hypothesize three reasons:

1. **Richer Supervision**: Hidden states contain more information than 
   output tokens, providing a denser learning signal across layers.

2. **Regularization Effect**: Multi-layer alignment acts as implicit 
   regularization, preventing the student from overfitting to specific 
   token patterns.

3. **Ensemble-like Behavior**: Distilling from multiple layers creates 
   an ensemble effect, averaging teacher knowledge across depth.

This phenomenon mirrors findings in multi-teacher distillation [cite], 
where students can outperform individual teachers.
```

#### 6.2 Limitations (1/4 page)

**Text:**
```
Our work has several limitations:

â€¢ **Domain Specificity**: We focus on mathematical reasoning. Results may 
  differ for other domains (e.g., generation, translation).

â€¢ **Teacher Dependency**: Performance is bounded by teacher quality. If 
  the teacher fails to learn latent reasoning, distillation cannot recover.

â€¢ **Computational Overhead**: Extracting hidden states during training 
  adds ~20% overhead compared to standard distillation.

â€¢ **Layer Selection**: We manually choose layers L={8-12}. Automatic 
  layer selection could improve results.

Future work should address these limitations through adaptive layer 
selection and multi-domain evaluation.
```

---

### â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
### 7. CONCLUSION (1/4 page)
### â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

**Text:**
```
We introduced LRD-PEFT, a novel framework combining latent reasoning 
distillation with parameter-efficient fine-tuning. By aligning hidden 
state representations across layers using LoRA adapters, our method 
achieves superior performance to both token-level distillation and full 
fine-tuning while training only 0.12% of parameters.

Key findings include:
â€¢ Latent distillation provides richer supervision than token-level methods
â€¢ PEFT can efficiently transfer complex reasoning capabilities
â€¢ Multi-layer alignment captures hierarchical reasoning patterns
â€¢ Student models can surpass teacher performance through implicit ensemble

Our work demonstrates that efficient knowledge transfer is possible without 
sacrificing quality, opening new directions for deploying reasoning 
capabilities at scale. Future work will explore automatic layer selection, 
multi-teacher distillation, and application to other domains beyond 
mathematical reasoning.
```

---

## ğŸ¨ SUMMARY: ALL FIGURES & TABLES

### Figures (7 total):
1. **Figure 1**: Motivation - Comparison of approaches (Page 2)
2. **Figure 2**: Overall framework (3 phases) (Page 3)
3. **Figure 3**: Loss dynamics (training curves) (Page 4)
4. **Figure 4**: LoRA architecture diagram (Page 4)
5. **Figure 5**: Distillation weight analysis (Page 6)
6. **Figure 6**: Efficiency comparison (3 subplots) (Page 6)
7. **Figure 7**: Hidden state analysis (t-SNE + similarity) (Page 7)

### Tables (6 total):
1. **Table 1**: Dataset statistics (Page 5)
2. **Table 2**: Baseline configuration summary (Page 5)
3. **Table 3**: Main results (all benchmarks) (Page 6)
4. **Table 4**: Ablation study (Page 6)
5. **Table 5**: Error analysis breakdown (Page 7)
6. **Table 6**: Generalization results (Page 7)

---

## ğŸ“Š VISUALIZATION BEST PRACTICES

### Figure Design Guidelines:
1. **Keep it simple**: One message per figure
2. **Use color sparingly**: 2-3 colors max
3. **Label everything**: Axes, legends, captions
4. **High contrast**: Black/white + one accent color
5. **Vector graphics**: Use PDF/SVG, not PNG
6. **Font size**: At least 10pt when rendered

### Table Design:
1. **Horizontal lines only**: Top, header, bottom
2. **Bold best results**: Easy to spot
3. **Align numbers**: Right-align numerical data
4. **Units in header**: Not repeated in cells
5. **Caption below**: Explain what's shown

---

## âœ… NeurIPS CHECKLIST ITEMS

Your paper must include:
- [ ] Claims supported by experiments
- [ ] Limitations discussed
- [ ] Broader impact considered
- [ ] Code/data availability stated
- [ ] Compute requirements disclosed
- [ ] Reproducibility information
- [ ] Theoretical assumptions stated (if applicable)
- [ ] Ethics guidelines followed

---

## ğŸ“ PAGE BUDGET ALLOCATION

```
Section              â”‚ Pages â”‚ % of Total
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Abstract             â”‚ 0.1   â”‚  1%
Introduction         â”‚ 1.2   â”‚ 13%
Related Work         â”‚ 1.0   â”‚ 11%
Method               â”‚ 2.3   â”‚ 26%
Experimental Setup   â”‚ 1.0   â”‚ 11%
Results & Analysis   â”‚ 2.3   â”‚ 26%
Discussion           â”‚ 0.5   â”‚  6%
Conclusion           â”‚ 0.25  â”‚  3%
Figures/Tables       â”‚ 0.35  â”‚  4%
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TOTAL CONTENT        â”‚ 9.0   â”‚ 100%
References           â”‚ 1-2   â”‚ (extra)
Appendix             â”‚ 2-3   â”‚ (extra)
Checklist            â”‚ 1     â”‚ (extra)
```

**Total PDF**: ~13-15 pages (9 content + extras)

---

This structure follows NeurIPS format exactly and provides a complete roadmap for your paper!
